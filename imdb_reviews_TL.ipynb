{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imdb_reviews_TL.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "J3fiZeyRFw5J",
        "OKd7pBw7C9qm"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPwiIScvy5La116GrPHtxMw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iasad1/NLP/blob/main/imdb_reviews_TL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBYUc5bcGHwx"
      },
      "source": [
        "# Building a sentiment analyser: Applying Transfer Learning\n",
        "\n",
        "With availability of neural networks, concepts such as stemming and lemmatisation are not necessary. In fact, neural networks may miss out learning subtlities taken away from stemming or lemmatisation.\n",
        "\n",
        "In this notebook, we will build a sentiment classifier that has been trained on IMDb dataset and then apply transfer learning to detect sentiments within our own dataset.\n",
        "\n",
        "There are four steps involved in the process,\n",
        "\n",
        "1. Reading and viewing the IMDb data\n",
        "2. Getting our own data for modelling\n",
        "3. Fine-tuning a language model\n",
        "4. Building the classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3fiZeyRFw5J"
      },
      "source": [
        "## About Language model\n",
        "\n",
        "Language model is an example of a pre-trained text based model that is good at predicting the next word that is likely to be typed. \n",
        "\n",
        "It has a good understanding of english, what's a happy word or sad word etc.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYoCpmtDDjmi"
      },
      "source": [
        "## Steps\n",
        "\n",
        "1. Get pre-processed data\n",
        "2. Create a language model with pre-trained weights that you can fine-tune\n",
        "3. Create other models like classificer on top of the encoder of the language model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5D5026NNq79"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBAYyF11Nra1"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHYu5sq0Nf0s"
      },
      "source": [
        "from fastai.text import *"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "id": "wpqm-RHgNuBX",
        "outputId": "8bc4c63b-8b5d-4360-ef6f-4d5cd7974c84"
      },
      "source": [
        "path = untar_data(URLs.IMDB_SAMPLE)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://files.fast.ai/data/examples/imdb_sample.tgz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S1OLB5hOKIZ"
      },
      "source": [
        "# path??"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5aKWZQOObzU",
        "outputId": "c0f3e1ee-825a-40c9-ac97-56c62a774209"
      },
      "source": [
        "path"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/root/.fastai/data/imdb_sample')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKd7pBw7C9qm"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "Unlike images, you cannot upload a text data simply into a pre-trained model. For text, you will have to transform it into a  list of words, tokens and then transform these tokens in to numbers. It is these numbers that are passed to embedding layers that will convert them in arrays of floats before passing through the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZfXV_KwOhCU"
      },
      "source": [
        "df =  pd.read_csv(path/'texts.csv')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4QNHSL7tOqpm",
        "outputId": "9084ec07-e36b-484f-e62f-dae9f81eba5e"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>is_valid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negative</td>\n",
              "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>This is a extremely well-made film. The acting...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>Every once in a long while a movie will come a...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>Name just says it all. I watched this movie wi...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>This movie succeeds at being one of the most u...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      label                                               text  is_valid\n",
              "0  negative  Un-bleeping-believable! Meg Ryan doesn't even ...     False\n",
              "1  positive  This is a extremely well-made film. The acting...     False\n",
              "2  negative  Every once in a long while a movie will come a...     False\n",
              "3  positive  Name just says it all. I watched this movie wi...     False\n",
              "4  negative  This movie succeeds at being one of the most u...     False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvWC1HMmOrzW",
        "outputId": "2494fafa-da47-4060-f8b5-22d85a9da138"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEfruZ0fQIh5"
      },
      "source": [
        "##  Create a `TextDataBunch` suitable for training a language model.\n",
        "\n",
        "\n",
        "Get the data and create a 'databunch' object to use for a language model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "tJDlIG84SP_g",
        "outputId": "51f678f9-857e-40ca-e1b3-0b01e417ce3a"
      },
      "source": [
        "%%time\n",
        "\n",
        "# throws `BrokenProcessPool` Error sometimes. Keep trying `till it works!\n",
        "count = 0\n",
        "error = True\n",
        "while error:\n",
        "    try: \n",
        "        # The following line throws `AttributeError: backwards` on the learning step, below\n",
        "        # data_lm = TextDataBunch.from_csv(path, 'texts.csv')\n",
        "        # This Fastai Forum post shows the solution:\n",
        "        #      https://forums.fast.ai/t/backwards-attributes-not-found-in-nlp-text-learner/51340?u=jcatanza\n",
        "        # We implement the solution on the following line:\n",
        "        data_lm = TextLMDataBunch.from_csv(path, 'texts.csv')\n",
        "        error = False\n",
        "        print(f'failure count is {count}\\n')    \n",
        "    except: # catch *all* exceptions\n",
        "        # accumulate failure count\n",
        "        count = count + 1\n",
        "        print(f'failure count is {count}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return np.array(a, dtype=dtype, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "failure count is 0\n",
            "\n",
            "CPU times: user 411 ms, sys: 82.1 ms, total: 493 ms\n",
            "Wall time: 35.8 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XK1Zq8BSfCS"
      },
      "source": [
        "## Create `databunch` object for use in a classifier object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "SItQGAG-SlKT",
        "outputId": "42eedc8b-76ba-4bb6-8864-f11c2c5e59d5"
      },
      "source": [
        "%%time\n",
        "\n",
        "# throws `BrokenProcessPool` Error sometimes. Keep trying `till it works!\n",
        "count = 0\n",
        "error = True\n",
        "while error:\n",
        "    try: \n",
        "        # Create the databunch for the classifier model\n",
        "        data_clas = TextClasDataBunch.from_csv(path, 'texts.csv', vocab=data_lm.train_ds.vocab, bs=32)\n",
        "        error = False\n",
        "        print(f'failure count is {count}\\n')    \n",
        "    except: # catch *all* exceptions\n",
        "        # accumulate failure count\n",
        "        count = count + 1\n",
        "        print(f'failure count is {count}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return np.array(a, dtype=dtype, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "failure count is 0\n",
            "\n",
            "CPU times: user 396 ms, sys: 82.5 ms, total: 479 ms\n",
            "Wall time: 35.7 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8beSaxPTHpx"
      },
      "source": [
        "## Save the `databunch` objects for use later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDSqHk7pTPAI"
      },
      "source": [
        "data_lm.save('data_lm_export.pkl')\n",
        "data_clas.save('data_cls_export.pkl')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9Fg506-ThDp",
        "outputId": "b32eaa1a-f651-4589-aa66-bc0125cdbbcf"
      },
      "source": [
        "type(data_lm)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "fastai.text.data.TextLMDataBunch"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "893OCroXJ-Uq"
      },
      "source": [
        "## Load the `databunch` objects for use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JybiMIBXKHD5",
        "outputId": "e0378d86-adb9-4066-e1f2-dfd652b63fab"
      },
      "source": [
        "data_lm = load_data(path,'data_lm_export.pkl',bs = 32)\n",
        "data_clas = load_data(path, 'data_cls_export.pkl',bs =32)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kvQ8aNTNFne"
      },
      "source": [
        "# IMDb review 'writer': Build and fine-tune a language model\n",
        "\n",
        "The idea is to create a learner out of a pre-trained model and then push through it our data_lm object. Later, we fine-tune a hyperparameters.\n",
        "\n",
        "Using fastai, we will download a model with an AWD_LSTM architecture with it's pretrained weights. Train it on the available data and then fine-tune."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YreRwu6fNUO1"
      },
      "source": [
        "### Set up to use the GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ydewq9yNYm2"
      },
      "source": [
        "torch.cuda.set_device(0)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDz9UP5bNddh"
      },
      "source": [
        "# torch.cuda??"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRNJenO1OB8I"
      },
      "source": [
        "## Build the IMDb language model\n",
        "\n",
        "This language model has been pre-trained on wikitext with hypertrained weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "id": "W7zp71FHOXXf",
        "outputId": "aab11942-12df-4e4e-b009-f35e78029949"
      },
      "source": [
        "learn = language_model_learner(data_lm,AWD_LSTM,drop_mult=.5)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://s3.amazonaws.com/fast-ai-modelzoo/wt103-fwd.tgz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf9Q9YAqWWa9"
      },
      "source": [
        "# language_model_learner??  "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDTa6-IzW3-O"
      },
      "source": [
        "## Start training\n",
        "\n",
        "Now that the language model object has been created, we will train it on our own data. As the first step, the model by default unfreezes the final layer, using the weights of the pre-trained set up for the initial layer.\n",
        "\n",
        "Once done, we will then unfreeze all the layers with differing learning rates in the first and final; with first layer learning rates set to low."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "L9lLiNgaXa8R",
        "outputId": "a0fb251c-b068-426e-ac0f-bf8d4fbe30b6"
      },
      "source": [
        "learn.fit_one_cycle(1,1e-2)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.320063</td>\n",
              "      <td>3.903686</td>\n",
              "      <td>0.284298</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCU1AqF2YCDU"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOjgtuIuYChI"
      },
      "source": [
        "## Unfreeze the weights and train all the layers\n",
        "Here we will be unfreezing all the layers while feeding the IMDB reviews. It is to be remembered, that given the model was trained on wiki-text the initial layers are already well optimised. Therefore, when learning with new data it is good to slow the learning rate of the earlier layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "zosaoDwxXtqj",
        "outputId": "5f62e4c4-6080-4137-9a56-776c9d725dec"
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(5,slice(1e-4,1e-2))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.933365</td>\n",
              "      <td>3.866877</td>\n",
              "      <td>0.287485</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.785280</td>\n",
              "      <td>3.905602</td>\n",
              "      <td>0.283836</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.367502</td>\n",
              "      <td>3.957690</td>\n",
              "      <td>0.281065</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.865020</td>\n",
              "      <td>4.035101</td>\n",
              "      <td>0.277602</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.506804</td>\n",
              "      <td>4.093227</td>\n",
              "      <td>0.276370</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY45KK-SX25t"
      },
      "source": [
        "# learn.fit_one_cycle??"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rQln-O5oUoW0",
        "outputId": "98e9b8a9-b033-4eb9-ba3b-75b1d491469f"
      },
      "source": [
        "learn.predict(\"This is a review about\",n_words=10)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This is a review about \" The Answer \" , a remake of'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "id": "RjuGChv5U29h",
        "outputId": "ac43ad93-f91c-40f7-f864-e5ec4963e0da"
      },
      "source": [
        "learn.predict(\"When it comes to direction\", n_words=100)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"When it comes to direction Mr. Head is a delicate . He works with the nuns who came to his office to handle the assassination attempt . He works for the dream bank and , despite some designs of art , not more than flawless . As an genius piece Hollywood 's co - funniest and every otherwise atmospheric film seems somewhat charming . i do n't know if this film was made for TV even though it was when it was shown on television . But if more than two other movies have been made for\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNFDa805WRpW"
      },
      "source": [
        "### Save the encoder\n",
        "\n",
        "As seen above, the quality of the predicted text isn't great. This is because the model has been trained on a very limited number of IMDb reviews. As the training corpus increases, this is expected to improve.\n",
        "\n",
        "For now, we will save the encoder to be able to use it for the classification task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFxzZ2j9WUfH"
      },
      "source": [
        "learn.save('mini_imdb_language_model')\n",
        "learn.save_encoder('mini_imdb_language_model_encoder')\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT9eRmegZGW_"
      },
      "source": [
        "## Building the review sentiment classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a139rW7mYnW_"
      },
      "source": [
        "learn = text_classifier_learner(data_clas,AWD_LSTM,drop_mult=.5).to_fp16()\n",
        "\n",
        "# We use mixed precision (.to_fp16())for greater speed, smaller memory footprint, and a regularizing effect."
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTxQFhzXbZYy",
        "outputId": "934bfc26-a7c5-4fee-98bc-3ca2ea26452a"
      },
      "source": [
        "learn.load_encoder('mini_imdb_language_model_encoder')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (799 items)\n",
              "x: TextList\n",
              "xxbos xxup the xxup shop xxup around xxup the xxup corner is one of the xxunk and most feel - good romantic comedies ever made . xxmaj there 's just no getting around that , and it 's hard to actually put one 's feeling for this film into words . xxmaj it 's not one of those films that tries too hard , nor does it come up with the xxunk possible scenarios to get the two protagonists together in the end . xxmaj in fact , all its charm is xxunk , contained within the characters and the setting and the plot ... which is highly believable to xxunk . xxmaj it 's easy to think that such a love story , as beautiful as any other ever told , * could * happen to you ... a feeling you do n't often get from other romantic comedies , however sweet and heart - warming they may be . \n",
              " \n",
              "  xxmaj alfred xxmaj xxunk ( xxmaj james xxmaj stewart ) and xxmaj xxunk xxmaj xxunk ( xxmaj margaret xxmaj xxunk ) do n't have the most xxunk of first xxunk when she arrives in the shop ( xxmaj xxunk & xxmaj co. ) he 's been working in for the past nine years , asking for a job . xxmaj they clash from the very beginning , mostly over a cigarette box that plays music when it 's opened -- he thinks it 's a ludicrous idea ; she makes one big sell of it and gets hired . xxmaj their bickering takes them through the next six months , even as they both ( xxunk , of course ! ) fall in love with each other when they share their souls and minds in letters passed through xxup xxunk xxmaj box xxunk . xxmaj this would be a pretty thin xxunk to base an entire film on , except that xxup the xxup shop xxup around xxup the xxup corner is xxunk xxunk - out with a brilliant supporting cast made up of entirely engaging characters , from the xxunk but lonely xxmaj xxunk xxmaj xxunk ( xxmaj frank xxmaj xxunk ) himself , who learns that his shop really is his home ; xxmaj xxunk ( xxmaj felix xxmaj xxunk ) , xxmaj xxunk 's sidekick and friend who always xxunk out of the room when faced with the possibility of being asked for his honest opinion ; xxunk xxunk - du - xxunk xxmaj xxunk ( xxmaj joseph xxmaj xxunk ) who ultimately gets his comeuppance from a xxunk righteous xxmaj xxunk ; and ambitious xxunk boy xxmaj xxunk xxmaj xxunk ( xxmaj william xxmaj tracy ) who wants nothing more than to be promoted to the position of xxunk for xxmaj xxunk & xxmaj co. xxmaj the xxunk love story between ' xxmaj dear xxmaj friends ' is played out in this little shop in xxmaj xxunk , xxmaj xxunk , in which xxmaj xxunk 's xxunk xxunk and xxunk xxunk to shop manager help the two xxunk - to - be along . xxmaj it 's nice that everyone gets a story in this film ; the supporting characters are well - developed , and xxmaj xxunk 's own journey in life is almost as touching as the one xxmaj alfred and xxmaj xxunk share . xxmaj his xxunk to new xxunk boy xxmaj xxunk ( xxmaj charles xxmaj smith ) for xxmaj christmas xxmaj eve dinner , made in the xxunk , beautiful snow of a xxmaj hungarian winter , makes the audience glad that he is not alone ; we come to care even for the characters whose love story it is n't this film 's business to tell . \n",
              " \n",
              "  xxmaj aside from the love story , i must say that xxmaj james xxmaj stewart is truly one of the best things about this film . xxmaj he does n't play the full - xxunk xxmaj jimmy xxmaj stewart persona in this film ( c / f ' xxmaj mr xxmaj smith xxmaj goes xxmaj to xxmaj washington ' for that ) ; in fact xxmaj alfred xxmaj xxunk is xxunk and abrupt and not particularly kind . xxmaj he 's rather a xxunk man , in fact , with little hint ( until , perhaps , the very end ) of the xxunk - xxunk down - home xxunk charm xxmaj stewart would soon come to xxunk . xxmaj when he finds out before xxmaj xxunk that they have been xxunk in secret , in fact , xxmaj xxunk does n't ' xxunk up -- he xxunk it out to see how far he can take the xxunk , especially since he quickly xxunk ( given his xxunk relationship with xxmaj xxunk as boss and xxunk ) that loving the person he knows through the xxunk letters might not xxunk with loving the person herself . xxmaj his description to xxmaj xxunk of the fictional xxmaj xxunk xxmaj xxunk ( what a name ! ) who was to become her xxunk is hilarious in the extreme , but also his way of proving that the letters do n't reveal all there is to a man , just as her letters do n't reveal all there is to her . xxmaj stewart plays this role perfectly -- he keeps his face perfectly controlled whenever xxmaj xxunk insults xxmaj mr. xxmaj xxunk , as she is often wo nt to do , even ( and especially ) to his face . xxmaj and yet one believes , underneath the xxunk and xxunk , that he * could * reveal his identity with as much xxunk and xxunk and sheer * hope * as he eventually does . \n",
              " \n",
              "  xxmaj special mention must be given to the other members of the cast as well . xxmaj margaret xxmaj xxunk xxunk rather less well in the first half of the film , but she really comes into her own in the closing - shop scene on xxmaj christmas xxmaj eve , when she almost gets her heart broken again by xxmaj alfred 's most vivid description of her xxunk xxunk . xxmaj frank xxmaj xxunk turns in a great performance as the jealous xxmaj xxunk xxmaj xxunk driven to nervous breakdown , the man who has to xxunk his meaning in life when he xxunk that his wife of 22 years does not want to ' grow old with him ' . xxmaj and xxmaj felix xxmaj xxunk plays the role of the xxunk but xxunk xxmaj xxunk wonderfully ( a xxmaj xxunk regular , since he appears as a hilarious xxmaj russian xxunk in xxunk particular note is the scene in which he helps his good friend xxmaj alfred get the xxmaj christmas present the latter * really * wants ... a wallet instead of that ludicrous cigarette box xxmaj xxunk is so hung up on . \n",
              " \n",
              "  xxmaj xxunk xxmaj xxunk really does himself proud with this film -- for example , the xxunk xxunk and xxunk care given to detail in the creation of the xxmaj xxunk shop is well worth the effort , right down to the xxmaj hungarian names on the door , the xxunk and the cash xxunk and so on . xxmaj but even though xxmaj xxunk chose to have the story set in xxmaj xxunk , the setting is actually universal : it could happen anywhere ; it could happen to you . xxmaj xxunk lies the charm of this simple story , these believable characters who really * are * people . xxmaj the snow on xxmaj christmas xxmaj eve is real as well , or at least as real as xxmaj xxunk could make it ( he had snow machines brought in at great expense ) . xxmaj it is this desire to make everything appear as real as possible that helps make the story even more believable , that gives this entire film a dreamy realism that can not be xxunk . ( xxmaj no , not even in a remake like xxup xxunk xxup got xxup mail . ) \n",
              " \n",
              "  * xxmaj this * is really the xxmaj jimmy xxmaj stewart xxmaj christmas film that people are missing out on when they talk about xxup it xxup 's a xxup wonderful xxup life . xxmaj not to xxunk from the merits of that other film , but there 'd be no harm , and in fact a lot of good , done in watching xxup the xxup shop xxup around xxup the xxup corner this xxmaj christmas instead . xxmaj it 's sweet , funny , charming , and xxmaj stewart is xxunk in his role . xxmaj we should all be so lucky as to have the romance depicted in this film ; the best thing about this film is that we come away from it feeling that we very possibly could .,xxbos xxmaj does any one know what the 2 sports cars were ? i think xxmaj robert xxmaj stack 's might have been a xxmaj xxunk . xxmaj rock xxmaj hudson 's character told his father he was taking a job in xxmaj iraq , is n't that xxunk ? i have had xxmaj xxunk xxmaj malone in my xxunk bank most of my life , maybe this was the film that impressed me . xxmaj loren xxmaj bacall sure did have some chops in this film and probably out - acted xxmaj malone but xxmaj xxunk 's part made a more xxunk impact so she got the xxmaj oscar for best supporting role . xxmaj was xxmaj loren 's part considered a leading xxunk man xxmaj hadley character was was probably a pretty common picture of xxunk of his era in that he was a regular guy who made it big in an emerging industry but in building a whole town he had forgotten his children to have his wife bring them up . xxmaj in time , being widowed he realized that they were all he really had and they were spoiled rotten , looking for attention , so rather than try to relate to his children he blew his head off . xxmaj an ancient morality tale . xxmaj but seriously , what were those sports cars ?,xxbos i 've watched it plenty of times and i 'm planning on buying the full feature . i love all of xxmaj jason xxmaj xxunk 's comedy . xxmaj it 's very different and unique and is very enjoyable . i love indie films and this one is just great . xxmaj the plot is strange but very funny . xxmaj this short film is about a talking xxmaj spatula named xxmaj edward . xxmaj the order of the events are a bit xxunk , making this film very interesting to watch . xxmaj at first you see xxmaj edward fighting the spoons , but then the focus changes to earlier in his life . xxmaj this is a silly movie , but of course , it 's still great . i highly recommend that you watch this film at xxunk or xxunk . xxmaj it 's very funny . xxmaj the humour may not match everybody 's taste but watch anyway . xxmaj it 'll only take 16 minutes of your time , and it 's free . xxup go xxup watch xxup spatula xxup madness !,xxbos xxmaj this show is unbelievable in that . . . what it xxunk and what it focuses on and . . . words can not describe how insane xxup et is . xxmaj they will report anything . xxmaj if a celebrity is even remotely xxunk connected to the story xxup et will report on it . xxmaj if a dog xxunk in the xxmaj tom xxmaj cruise 's yard they will report on it . xxmaj if a celebrity dies . . . they will talk about it for weeks on end to the point where the public xxunk that celebrity . xxmaj if a celebrity is on trial . . . xxup et will report it for xxup months on end . xxmaj there is no end to what this show will reports and no time frame that xxunk how long they will focus on a story . xxmaj is it even considered xxunk xxunk ? xxmaj the reports are so xxunk annoying too , with harsh rambling voices and end with an unnecessary xxunk to convey a sense of important . i can not watch this show without xxunk humanity 's existence . xxup et is one big reason i avoid pre - evening shows in general . i regret that imdb can only allow a minimum of one star rating and not zero or even in the xxunk . xxmaj for this show deserve xxunk xxmaj stars .,xxbos xxmaj well , it is hard to add comment after reading what is already here but i feel i must say something . i was n't exactly looking for ' a xxunk ' as someone puts it or even ' blood and guts / gore ' . i have some respect for the victims xxunk although i really felt the xxunk xxup xxunk xxunk were xxunk , faceless and meaningless . xxmaj just a xxunk for xxmaj xxunk sexual antics . \n",
              " \n",
              "  i watched this film with the kind of morbid curiosity that makes me think ' xxmaj what makes a guy be a serial killer ? ' as well as wondering the specifics about the xxmaj dahmer story , of which i know very little . xxmaj people here seem to think that the movie did n't have to cover the events of the xxmaj dahmer story .. xxup i.e. his history , what happened when he got caught , the aftermath , etc but xxup it xxup is xxup important ! xxmaj you see , i assume if you are xxmaj american you xxup will xxup know all of this . xxmaj we do not all live in xxmaj america . xxmaj to tell this story about such a man as he obviously was , xxup requires that at least xxup some of the history and actual events are told / shown . xxmaj this does n't mean blood and guts , there are ways of showing horrific things in a movie by implication or clever filming without resorting to gore . xxmaj without even touching upon some of what he did ( i found out more about him reading the user comments on this site ! ) , the movie felt like a void . a moment in time with very little substance . i would like to know if there is a film about the xxup real xxmaj dahmer because with its lack of direction , xxup very slow pace that xxup never changes , xxmaj strange portrayal of xxunk and the xxup very unfortunate lack of xxup any attempt at an ending , this movie is xxup poor . i would not recommend to anyone that they waste the time it takes to watch it . a definite 1 out of 10 ( for the acting ! )\n",
              "y: CategoryList\n",
              "positive,positive,positive,negative,negative\n",
              "Path: /root/.fastai/data/imdb_sample;\n",
              "\n",
              "Valid: LabelList (201 items)\n",
              "x: TextList\n",
              "xxbos xxmaj as part of the celebration of the release of xxmaj casino xxmaj xxunk , this film with the new xxmaj bond starring in it was shown , from director xxmaj roger xxmaj michell ( xxmaj xxunk xxmaj hill ) . i almost turned it off for being a bit boring , but i 'm glad i stuck with it . xxmaj basically xxmaj may ( xxmaj anne xxmaj xxunk ) is a single mother of xxmaj helen ( xxmaj anna xxmaj wilson - xxmaj jones ) who hardly sees anyone and has not had a boyfriend in years . xxmaj her daughter says that she might want to get married to her new boyfriend , xxmaj darren ( xxmaj daniel xxmaj craig , of course ) . xxmaj after knowing each other only a few days , xxmaj may and xxmaj darren have a secret affair . xxmaj and at her age , with a xxunk , and the new xxmaj bond ? ! xxmaj anyway , they obviously want to keep it a secret , but xxmaj may has regrets and wonders if xxmaj helen will find out . xxmaj when she does , xxmaj darren gets less xxunk than xxmaj xxunk xxmaj in fact , xxmaj helen asks her xxunk to hit her . xxmaj also starring xxmaj peter xxmaj xxunk as xxmaj xxunk , xxmaj xxunk xxmaj xxunk as xxmaj xxunk xxmaj pair , xxmaj harry xxmaj michell as xxmaj harry , xxmaj rosie xxmaj michell as xxmaj rosie and xxmaj johnny xxmaj english 's xxmaj oliver xxmaj ford xxmaj davies as xxmaj bruce . xxmaj very good !,xxbos xxmaj if you 're as huge of a fan of an author as i am of xxmaj jim xxmaj thompson , it can be pretty dodgy when their works are xxunk to film . xxmaj this is not the case with xxmaj scott xxmaj xxunk 's rendition of xxup after xxup dark xxup my xxup sweet . a suspenseful , sexually xxunk noir classic that closely follows and does great justice to the original text . xxmaj jason xxmaj patrick and xxmaj rachel xxmaj ward give possibly the best performances of their careers . xxmaj and the always phenomenal xxmaj bruce xxmaj dern might have even xxunk him self with this one . xxmaj like xxmaj thompson 's book this movie creates a dark and surreal world where passion xxunk logic and the double cross is never far at hand . a must see for all fans of great noir film . xxrep 4 * ! ! !,xxbos i had read many good things about this adaptation of my favorite novel ... so invariably my expectations were crushed . xxmaj but they were crushed more than should be expected . xxmaj the movie would have been a decent movie if i had not read the novel beforehand , which perhaps ruined it for me . \n",
              " \n",
              "  xxmaj in any event , for some reason they changed the labor camp at xxmaj xxunk to a ship full of xxunk slaves . xxmaj the scene at xxmaj bishop xxmaj xxunk 's was fine . xxmaj in fact , other than the xxunk , things survived up until the xxunk of xxmaj fantine . xxmaj because we do not want to have bad things happen to a good woman , she does not cut her hair , sell her teeth , or become a prostitute . xxmaj the worst she does is run into the mayor 's office and spit on his face . xxmaj xxunk is entirely xxunk . xxmaj because having children out of xxunk should also not be talked about , xxmaj xxunk is xxmaj fantine 's dead husband , rather than an irresponsible dandy . xxmaj valjean is able to xxunk xxmaj cosette for xxmaj fantine before the xxmaj xxunk affair , so they reunite happily , yet another change . xxmaj then comes the convent , which is a pretty difficult scene to screw up . xxmaj thankfully , it was saved . xxmaj after this three minutes of accuracy , however , the movie again begins to xxunk towards xxmaj classic xxmaj novel xxmaj xxunk . \n",
              " \n",
              "  xxmaj as xxmaj cosette and xxmaj valjean are riding through the park , they come across xxmaj marius giving a speech at a meeting . xxmaj about prison reform . xxmaj when he comes to hand out xxunk to xxmaj valjean and xxmaj cosette , he says the one line in the movie that set me screaming at the xxup tv set . \" xxmaj we are n't revolutionaries . \" i could hear xxmaj victor xxmaj xxunk xxunk in his grave . xxup of xxup course xxup they xxup are xxup revolutionaries ! xxmaj they want to revolt against the pseudo - xxunk that is in place in favor of another republic , you dumb screenwriters ! xxmaj it 's a historical xxup fact that there was an xxunk against the government in xxunk . \n",
              " \n",
              "  xxmaj at one point xxmaj cosette goes to give xxmaj marius a xxunk from her father for the reform movement and meets xxmaj eponine . xxmaj except ... not xxmaj eponine . xxmaj or at least not the xxmaj eponine of the book . xxmaj this xxmaj eponine appears to be a well - to - do secretary girl working for the prison reformers ( who are working out of the xxmaj cafe xxmaj universal as opposed to the xxmaj cafe xxmaj xxunk ) . xxmaj not to mention the audience is already made to dislike her thanks to her not - period , low - cut , tight - fitting dress and her xxunk mannerisms . \n",
              " \n",
              "  xxmaj the prison reformers ( xxmaj lead by the most poorly cast xxmaj enjolras that i have xxup ever seen ) decide that xxunk out xxunk is n't good enough anymore . xxmaj so they 're going to build barricades . i do n't know about you , but i have never heard of reform movements tearing up the streets and building barricades and attacking government troops . xxmaj about three hundred people ( it was not supposed to be so many ) start attacking the xxmaj national xxmaj guard and building a bunch of barricades , etc . xxmaj eponine does die for xxmaj marius , thankfully . \n",
              " \n",
              "  xxmaj the rest of the movie is sort of accurate , except that xxmaj javert 's suicide again seems hard to understand thanks to his minuscule screen time and odd character interpretation . xxmaj the movie ends with xxmaj valjean watching xxmaj javert jump into the river . xxmaj this is again inaccurate because xxmaj valjean would never have let xxmaj javert xxunk . xxmaj he saved the man 's life earlier , why let him die now ? xxmaj then there 's the whole skipping of xxmaj valjean 's confession to xxmaj marius , his xxunk , and his redemption on his xxunk with xxmaj marius and xxmaj cosette by his side . \n",
              " \n",
              "  xxmaj overall , i can blame the script mostly for the problems . xxmaj while i am glad xxmaj enjolras and xxmaj eponine were at least present in the film , they were terribly xxunk , as was the entire barricade scene . xxmaj the xxunk of xxmaj fantine 's suffering prevents us from feeling too much pity for her . xxmaj that xxmaj cosette knows xxmaj valjean 's past from the start messes with the plot a good deal . i did not even see xxmaj thenardier , and xxmaj xxunk . xxmaj thenardier only had a few seconds of screen time . xxmaj the same with xxmaj xxunk . i did like xxmaj xxunk xxmaj march 's interpretation of xxmaj valjean a lot , however , which was one of the redeeming features of the movie . xxmaj on the other hand , xxmaj charles xxmaj xxunk , for all his great acting in other movies , seems to have missed the mark with xxmaj javert . xxmaj the lip xxunk , the unnecessary shouting , and his acting in general all just felt very wrong . xxmaj he also , like many xxmaj xxunk i have seen , did not appear at all menacing , something required of the character . \n",
              " \n",
              "  xxmaj again , this film would probably feel much better if i had not read the book . i would not recommend it to book purists , though . i would also say that the movie would have been a good adaptation for the time had not the xxunk accurate xxmaj french version come out the year before .,xxbos xxmaj where to start ? ! . . . i feel ... violated ! xxmaj that s right , violated ! i just spent 1.5hrs of my life , 1.5hrs that i could have spent doing something more useful , like watching paint dry , on this so called horror flick . \n",
              " \n",
              "  xxmaj its not scary , its not funny , its not dramatic , its no action , its nothing ... \n",
              " \n",
              "  xxmaj its predictable , its boring , its tragic ... \n",
              " \n",
              "  i might come of a bit harsh here , but watch this movie and you will feel the same way ... or ... no , do n't watch it ... unless you want to feel violated also .,xxbos xxmaj following the brilliant \" xxmaj xxunk \" ( aka . \" xxmaj xxunk xxmaj the xxmaj razor - xxmaj sword xxmaj of xxmaj justice \" , 1972 ) and its excellent ( and even xxunk ) sequel \" xxmaj xxunk : xxmaj xxunk xxmaj xxunk xxunk xxunk \" ( aka . \" xxmaj razor 2 : xxmaj the xxmaj xxunk \" , 1973 ) , this \" xxmaj xxunk : xxmaj xxunk no xxmaj xxunk xxunk xxunk \" aka . \" xxmaj razor 3 : xxmaj who 's xxmaj got xxmaj the xxmaj gold \" is the third , and sadly final installment to the awesome saga about the xxunk xxmaj samurai - xxunk xxmaj xxunk ' xxmaj the xxmaj razor ' xxmaj xxunk ( brilliantly played by the great xxmaj xxunk xxmaj xxunk ) , who fights xxunk with his fighting xxunk as well as his enormous sexual powers . xxmaj as a big fan of 70s exploitation cinema made in xxmaj xxunk , \" xxmaj sword xxmaj of xxmaj justice \" became an xxunk favorite of mine , and i was therefore more than eager to find the sequels , and full of anticipation when i finally stumbled over them recently . xxmaj while this third \" xxmaj xxunk \" film is just not quite as brilliant as its predecessors it is definitely another great piece of cult - cinema that no lover of xxmaj japanese exploitation cinema can afford to miss . \" xxmaj who 's xxmaj got xxmaj the xxmaj gold \" is a bit xxunk than the two xxunk xxmaj xxunk films , but it is just as brilliantly comical and xxunk humorous , and immediately starts out xxunk odd : xxmaj the film begins , when xxmaj xxunk 's two xxunk see a female ghost when xxunk . xxmaj having always wanted to sleep with a ghost , xxmaj xxunk insists that his xxunk lead him to the site of the xxunk ... xxmaj if that is not a promising beginning for an awesome film experience , i do n't know what is . xxmaj xxunk xxmaj xxunk , one of my personal favorite actors , is once again brilliant in the role of xxmaj xxunk , a role that seems to have been written specifically for him . xxmaj xxunk xxup is xxmaj xxunk , the xxunk and xxunk xxunk , who hates xxunk and deliberately insults his xxunk , and whose unique xxunk techniques include xxunk female suspects . xxmaj the xxunk women than immediately fall for him , due to his sexual powers and enormous penis , which he trains in a rather grotesque routine xxunk . i will not give away more about the plot in \" xxmaj who 's xxmaj got xxmaj the xxmaj gold \" , but i can xxunk that it is as cool as it sounds . xxmaj the supporting performances are also very good , and , as in the predecessors , there are plenty of xxunk xxunk characters . xxmaj this is sadly the last film in the xxunk sleazy ' xxmaj xxunk ' series . xxmaj if they had made 20 sequels more , i would have happily watched them all ! xxmaj the entire xxmaj xxunk series is brilliant , and while this third part is a bit xxunk compared to its predecessors , it is definitely a must - see for all lovers of cult - cinema ! xxmaj oh how i wish they had made more sequels !\n",
              "y: CategoryList\n",
              "positive,positive,negative,negative,positive\n",
              "Path: /root/.fastai/data/imdb_sample;\n",
              "\n",
              "Test: None, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(8968, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(8968, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7fd360189320>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/root/.fastai/data/imdb_sample'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
              "learn: ...\n",
              "alpha: 2.0\n",
              "beta: 1.0, MixedPrecision\n",
              "learn: ...\n",
              "loss_scale: 65536\n",
              "max_noskip: 1000\n",
              "dynamic: True\n",
              "clip: None\n",
              "flat_master: False\n",
              "max_scale: 16777216\n",
              "loss_fp32: True], layer_groups=[Sequential(\n",
              "  (0): Embedding(8968, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(8968, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "PXtcA6tLh7Bi",
        "outputId": "c55e0fcd-3c02-4118-d0d8-eff6750b9c2f"
      },
      "source": [
        "data_clas.show_batch()\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos xxmaj raising xxmaj victor xxmaj vargas : a xxmaj review \\n \\n  xxmaj you know , xxmaj raising xxmaj victor xxmaj vargas is like sticking your hands into a big , steaming bowl of xxunk . xxmaj it 's warm and gooey , but you 're not sure if it feels right . xxmaj try as i might , no matter how warm and gooey xxmaj raising xxmaj</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos xxup the xxup shop xxup around xxup the xxup corner is one of the xxunk and most feel - good romantic comedies ever made . xxmaj there 's just no getting around that , and it 's hard to actually put one 's feeling for this film into words . xxmaj it 's not one of those films that tries too hard , nor does it come up with</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos xxmaj now that xxmaj che(2008 ) has finished its relatively short xxmaj australian cinema run ( extremely limited xxunk screen in xxmaj xxunk , after xxunk ) , i can xxunk join both xxunk of \" xxmaj at xxmaj the xxmaj movies \" in taking xxmaj steven xxmaj soderbergh to task . \\n \\n  xxmaj it 's usually satisfying to watch a film director change his style /</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos xxmaj this film sat on my xxmaj xxunk for weeks before i watched it . i dreaded a self - indulgent xxunk flick about relationships gone bad . i was wrong ; this was an xxunk xxunk into the screwed - up xxunk of xxmaj new xxmaj yorkers . \\n \\n  xxmaj the xxunk is the same as xxmaj max xxmaj xxunk ' \" xxmaj la xxmaj ronde</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos xxmaj many neglect that this is n't just a classic due to the fact that it 's the first xxup 3d game , or even the first xxunk - up . xxmaj it 's also one of the first stealth games , one of the xxunk definitely the first ) truly claustrophobic games , and just a pretty well - rounded gaming experience in general . xxmaj with graphics</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzgIjeuqc086"
      },
      "source": [
        "### Training final layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "_Yq4UFDRc5Zn",
        "outputId": "b32987f9-4661-4973-ed04-ea7151c71b22"
      },
      "source": [
        "learn.fit_one_cycle(1,1e-2)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.574490</td>\n",
              "      <td>0.558551</td>\n",
              "      <td>0.741294</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suxckffgdF2S"
      },
      "source": [
        "### Unfreeze all the layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k5Efug5ddJVU",
        "outputId": "7bc454b1-e043-4b66-96d0-71ae65e48b61"
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(3,slice(1e-4,1e-2))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.440874</td>\n",
              "      <td>0.591832</td>\n",
              "      <td>0.761194</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.406676</td>\n",
              "      <td>0.395373</td>\n",
              "      <td>0.825871</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.324262</td>\n",
              "      <td>0.385808</td>\n",
              "      <td>0.840796</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk-MNJKaeScw"
      },
      "source": [
        "### Test the classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFE9pxwieYDQ",
        "outputId": "eeab518e-12ca-4570-fa67-e523c9a9780d"
      },
      "source": [
        "learn.predict('Although Joker as a movie had it shock moments, much of it felt manufactured')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Category tensor(1), tensor(1), tensor([0.4201, 0.5799]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gHpniPCFfHxh",
        "outputId": "b9f9dae9-1905-4773-bf8c-fe6fb6f8a7b6"
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(5, slice(1e-4, 1e-2))\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.094509</td>\n",
              "      <td>0.567058</td>\n",
              "      <td>0.825871</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.118626</td>\n",
              "      <td>0.757057</td>\n",
              "      <td>0.796020</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.130860</td>\n",
              "      <td>0.503329</td>\n",
              "      <td>0.835821</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.112839</td>\n",
              "      <td>0.547296</td>\n",
              "      <td>0.845771</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.082546</td>\n",
              "      <td>0.558863</td>\n",
              "      <td>0.845771</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1504: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IPHN-cZgOmW",
        "outputId": "c4878383-a81e-42b9-acae-a36f932eef8e"
      },
      "source": [
        "learn.predict('the joker is an incredible film')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Category tensor(1), tensor(1), tensor([2.1867e-04, 9.9978e-01]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    }
  ]
}